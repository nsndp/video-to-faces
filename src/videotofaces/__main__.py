import argparse
from .main import video_to_faces

class CustomHelpFormatter(argparse.HelpFormatter):
    
    def __init__(self, prog):
        super().__init__(prog, max_help_position=40, width=120)

    # https://stackoverflow.com/a/29485128
    def _split_lines(self, text, width):
        return super()._split_lines(text, width) + ['']

    # https://stackoverflow.com/a/31124505
    def _format_action_invocation(self, action):
        if not action.option_strings or action.nargs == 0:
            return super()._format_action_invocation(action)
        default = self._get_default_metavar_for_optional(action)
        args_string = self._format_args(action, default)
        return ', '.join(action.option_strings) + ' ' + args_string

parser = argparse.ArgumentParser(formatter_class=CustomHelpFormatter)

parser.add_argument('-i', '--input-path', metavar='PATH', help='Path to a video file, a directory with video files, or a .txt file with full paths inside.')
parser.add_argument('-e', '--input-ext', metavar='EXTENSIONS', help='If -i is a directory, process only files with this extension. Can contain multiple extensions separated by a semicolon.')
parser.add_argument('-o', '--out-dir', metavar='PATH', help='Path to the output directory. A folder named "faces" (and, if needed, "intermediate") will be created under it. Assumed to be the same as the input directory if omitted.')
parser.add_argument('-op', '--out-prefix', metavar='TEXT', default='', help='A prefix to add to every resulting image. Full naming will be "[prefix][k]_{f}_{i}.jpg", where {f} is a video\'s frame number from the start, {i} is a face number within that frame, and [k] is the 1-based index of the video in the input folder/list (absent if input is a single video).')
parser.add_argument('-s', '--style', metavar='TEXT', required=True, help='Whether the inputs are anime or live-action. Accepted values: "live", "anime". Necessary for choosing the proper Torch models.')
parser.add_argument('-m', '--mode', metavar='TEXT', default='full', help='Accepted values: "full", "detection", "grouping" (the default is "full", which is equivalent to calling "detection", then "grouping"). When using "grouping", specify either -o with face images or the same -i that was used during detection.')
parser.add_argument('-d', '--device', metavar='TEXT', help='Whether to use CPU or GPU for Torch models when detecting/grouping. Accepts everything that torch.device accepts (https://pytorch.org/docs/stable/tensor_attributes.html#torch.device). Defaults to "cuda:0" if a GPU is available and to "cpu" otherwise.')
parser.add_argument('--save-frames', action='store_true', help='Save every processed video frame to "<--out-dir>/intermediate/frames" for closer examination of the detector\'s performance. Boxes will be drawn around detected faces either in green (passed) or red (rejected), and the score (i.e. the detector\'s confidence for this particular face) will be written for each box. The frames will be saved with lower JPG quality (50%%) and downscaled to fit into a 1024x1024px box if necessary to avoid eating too much disk space.')
parser.add_argument('--save-rejects', action='store_true', help='Save every detected but rejected face (which will happen if it\'s smaller than --det-min-size, has lower score than --det-min-score or positioned closer to a frame border than --det-min-border) to "<out_dir>/intermediate/rejects" for closer examination. A file named "log_rejects.csv" will also be created at "intermediate", detailing the exact compared values for each rejected file.')
parser.add_argument('--save-dupes', action='store_true', help='Save every face that was detected but marked as duplicate to "<--out-dir>/intermediate/dupesN", along with a log file "log_dupesN.csv", where N will be 1, 2 or 3 depending on the dup-checking part (see --hash-thr for more about parts 1-2 and --enc-dup-thr for more about part 3).')
parser.add_argument('--video-step', metavar='SEC', type=float, default=1, help='Controls the interval (in seconds) at which video frames are taken for detection. The default is 1, i.e. every second (meaning every 30th frame for 30fps video, for example). Adjust this depending on how fast-changing your videos are.')
parser.add_argument('--video-fragment', metavar='MIN', type=float, nargs=2, help='Process only the specified segment of the input video. Expects two space-separated values in minutes, e.g. "1 1.5" will mean to only detect faces from 60 sec to 90 sec mark.')
parser.add_argument('--video-area', metavar='PX', type=int, nargs=4, help='Process only the specified rectangle area of the input video. Expects four space-separated values in pixels (x1, y1, x2, y2), where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner (with the top-left frame corner being (0, 0)). Helpful if you have a video with black bars (or some other background where faces never appear) to avoid seeing parts of them in results and also to save a bit on processing time. E.g. if a typical 1080p video has full-width, 140px-height black bars above and below, "0 140 1920 940" will exclude them.')
parser.add_argument('--video-reader', metavar='TEXT', default='opencv', choices=['opencv', 'decord'], help='Which library to use for video reading. Accepted values: "opencv", "decord". The default is "opencv", since Python\'s cv2 is required for image manipulation anyway, but you need "decord" if you want to use GPU for video reading (and have https://github.com/dmlc/decord with GPU support installed). If your CPU/GPU difference is big enough (like in Google Colab VMs), video reading is likely to become a big bottleneck, so it might be worth going through the trouble to choose the latter. Note that -d only affects Torch models and will have no effect here: for "decord", it will use GPU if possible and CPU otherwise; for "opencv", it will always use CPU (enabling GPU decoding for OpenCV was too much of a black magic for me).')
parser.add_argument('--det-model', metavar='TEXT', default='default', help='Which Torch model to choose for face detection. Accepted values: "yolo", "mtcnn" when -s="live"; "rcnn" when -s="anime". MTCNN might be a better choice if you\'re strapped for resources, but otherwise you might as well just leave this param undefined, which defaults to "yolo" and "rcnn" respectively.')
parser.add_argument('--det-batch-size', metavar='INT', type=int, default=4, help='How many frames to process at once when detecting faces. The default is 4. A bigger batch usually leads to faster results, but a batch too large will simply crash "after using all available RAM/VRAM". Can vary greatly depending on your hardware, but since input images are usually much larger for detection than for embedding (e.g. 608px or 1333px vs 192x192 or 128x128), this number is likely to be much smaller than --enc-batch-size.')
parser.add_argument('--det-min-score', metavar='FLOAT', type=float, default=0.4, help='Reject every face with a score (i.e. confidence metric as reported by the detection model) lower than this. Increase to filter out more false positives, decrease to detect as much as possible. The default is 0.4. Might be worth adjusting for a specific video after doing a "test pass" while setting --save-frames and examining frames with green/red boxes drawn on them (the same recommendation applies to --det-min-size and --det-min-border).')
parser.add_argument('--det-min-size', metavar='PX', type=int, default=50, help='Reject every face where either width or height (in pixels) is smaller that this. The default is 50. Detectors will get more false positives at smaller sizes, so might be worth adjusting in conjunction with --det-min-score. Note that enlargement from --det-scale happens after and thus has no effect here, i.e. the size check is against the raw boxes as returned by the detector model.')
parser.add_argument('--det-min-border', metavar='PX', type=int, default=5, help='Reject every face where any border is closer to a frame\'s border (in pixels) that this. The default is 5. Helpful for filtering out partial faces that continue off-screen, because detector\'s boxes for those will either be at the very border or go into negative space as well.')
parser.add_argument('--det-scale', metavar='N', type=float, nargs=4, default=[1.5, 1.5, 2.2, 1.2], help='Enlarge every detected face area using four scale factors (SL, SR, SU, SD): the left border will become SL times more distant from the image\'s center, the right border - SR times more distant, the upper border - SU times, the bottom border - SD times. Useful if you need heads or portraits rather than tightly-cropped faces. The default is "1.5 1.5 2.2 1.2", which usually translates into a decent portrait crop (especially for anime where hair and headwears are very important parts of characters, hence a bigger scale factor for the upper border), though it\'s really worth to play with these numbers for a particular video/detection model. Scale factors lower than 1 will mean contraction instead of expansion. Set to "1 1 1 1" if you want to keep detected boxes as-is.')
parser.add_argument('--det-square', action='store_true', help='Enlarge every detected face area so that the resulted image is a square. This happens after the enlargement from --det-scale. Note that if a face is close to the frame\'s border, the resulting area will be moved accordingly so it doesn\'t go overboard, meaning that any border faces might get visibly non-centered with this option.')
parser.add_argument('--hash-thr', metavar='INT', type=int, default=8, help='Controls the strictness of the duplicate check, i.e. whether the current face image is similar enough to any previous face image (if it does, it\'s marked as a duplicate and not included in the final results). Higher numbers mean bolder marking, i.e. 0-2 will mark only near-identical images, while 12-16 will spot more but lead to some false positives as well (the default is 8; set as "None" to remove the check altogether). The number\'s precise meaning is the difference between the images\' average hashes (which are more reliable compared to perceptual hashes of diff hashes, despite being the simplest ones), and the check happens in two parts, with part 1 comparing only to the previous 5 faces (since this is where most duplicates will show up in most videos, with scenes rarely changing every single second and all) and part 2 doing a second pass by comparing every remaining pair. Set --save-dupes and explore the logs if you want to fine-tune this parameter.')
parser.add_argument('--enc-model', metavar='TEXT', default='default', help='Which Torch model to choose for getting image embeddings used in grouping. Accepted values: "facenet_vgg", "facenet_casia" when -s="live"; "vit_b", "vit_l" when -s="anime". Facenet VGG is marginally more precise than CASIA; VIT-L is moderately better than VIT-B but has a much larger weight file (1.19 GB vs 336 MB). Defaults to "facenet_vgg" and "vit_b" respectively.')
parser.add_argument('--enc-batch-size', metavar='INT', type=int, default=16, help='How many images to process at once when getting the faces\' embeddings for grouping. The default is 16. A bigger batch usually leads to faster results, and since the inputs here are much smaller than for detectors, it\'ll probably be safe to set a much higher number on good hardware (e.g. 128 or 256).')
parser.add_argument('--enc-area', metavar='N', type=float, nargs=4, help='Allows to use only a certain rectangular area of face images to calculate their embeddings. Defined in decimals from 0 to 1 as (px1, py1, px2, py2), where (px1, py1) and (px2, py2) are the top-left and bottom-right corner of the area respectively (e.g. "0.2 0.1 0.8 0.9" for a 100x100px image would cut off 20 pixels from left and right + 10 pixels from top and bottom before passing it to the encoder model). It can help to set this as an inverse of --det-scale, because while you might prefer to have heads or portraits for saving, encoders could still perform better on raw tightly-cropped faces. The default is None, i.e. use images for embedding as-is.')
parser.add_argument('--enc-dup-thr', metavar='FLOAT', type=float, default=0.5, help='Controls the strictness of part 3 of the duplicate check, where the faces are compared not by their hashes but by euclidean distances between their embeddings (and every face that\'s closer than this number to some other face is marked as a  duplicate and not included in the final results). Higher numbers mean bolder marking. The default is 0.5. Set to "None" to disable part 3 altogether. Set --save-dupes and explore the logs if you want to fine-tune this parameter.')
parser.add_argument('--group-mode', metavar='TEXT', default='clustering', help='Accepted values: "clustering", "classification" (the default is "clustering"). The former would group the faces automatically using K-means clustering (a less reliable but more convenient option); the latter would sort them into pre-defined groups based on reference images provided at --ref-dir (a more precise option that requires extra prep).')
parser.add_argument('--clusters', metavar='TEXT', default='2-9', help='If --group-mode="clustering", this controls how many groups (clusters) you will get. Can be a single number, a comma-separated list of numbers or a range defined as "FROM-TO". For more than one number, every option will be checked, and the one with the highest silhouette score (i.e. most well-defined clusters) will be selected (unless --clusters_save_all is specified). The default is "2-9". The results will appear under "<--out-dir>/faces/<N>", where N is the cluster\'s 0-based index.')
parser.add_argument('--clusters-save-all', action='store_true', help='If --group-mode="clustering" and --clusters is more than one number, this will save grouping results for every possible cluster count under "<--out-dir>/faces/G<K>" so you can choose the best one manually instead of just relying on silhouette score. E.g. for 50 detected faces and --clusters="2,4,7" you will then have folders "G2","G4","G7" with 50 images each, split under subfolders "0","1"; "0","1","2","3" and "0","1","2","3","4","5","6" respectively.')
parser.add_argument('--random-state', metavar='INT', type=int, default=0, help='If --group-mode="clustering", this is a random state value as passed to sklearn KMeans function. The default is 0. Any value besides None will lead to reproducible results (for that particular value), i.e. exactly the same clustering outputs every time it is applied to the same input.')
parser.add_argument('--ref-dir', metavar='PATH', help='If --group-mode="classification", this needs to be a path to a folder with multiple subfolders (their names will become class names to group into), each with a single reference image inside. Every face will be assigned to a class with the closest ref image, where "the closest" is defined as "having the shortest euclidean distance between their embeddings" (unless --enc-oth-thr is not None, then there\'ll also be a class named "other"). Can be omitted if you have an appropriate "ref" folder inside --out-dir.')
parser.add_argument('--enc-oth-thr', metavar='FLOAT', type=float, default=1.5, help='If --group-mode="classification", a face will be assigned to "other" class if its distance to every ref image is larger than or equal to this number. The default is 1.5. Set to None to exclude "other" class completely.')
parser.add_argument('--group-log', action='store_true', help='Save a log file ("log_clustering.csv" or "log_classification.csv" depending on --group-mode) under "<--out-dir>/faces" with details of the grouping process for closer examination.')

args = parser.parse_args()
video_to_faces(**vars(args))